{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8495690,"sourceType":"datasetVersion","datasetId":5069298},{"sourceId":8495749,"sourceType":"datasetVersion","datasetId":5069344},{"sourceId":8497236,"sourceType":"datasetVersion","datasetId":5070442},{"sourceId":8497814,"sourceType":"datasetVersion","datasetId":5070835}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### In this notebook, a selection of samples is extracted for Part-of-Speech (POS) tagging and Named Entity Recognition (NER). These samples are then manually annotated by myself and two collaborators. The resulting annotations are evaluated against the model's predictions using performance metrics such as Accuracy and Cohen's Kappa to assess agreement and labeling quality.\n\n","metadata":{}},{"cell_type":"markdown","source":"#### Extracting Some samples to annotate manually","metadata":{}},{"cell_type":"code","source":"merged_df = pd.read_csv('/kaggle/input/mergedlyircs-plus/kaggle/working/merged_plus_decade_and_coref.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:32:48.700413Z","iopub.execute_input":"2024-05-23T14:32:48.700761Z","iopub.status.idle":"2024-05-23T14:33:05.329213Z","shell.execute_reply.started":"2024-05-23T14:32:48.700734Z","shell.execute_reply":"2024-05-23T14:33:05.328213Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/2602866256.py:1: DtypeWarning: Columns (8,11) have mixed types. Specify dtype option on import or set low_memory=False.\n  merged_df = pd.read_csv('/kaggle/input/mergedlyircs-plus/kaggle/working/merged_plus_decade_and_coref.csv')\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### POS Tags Extraction","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npos_df1 = pd.read_csv('/kaggle/input/lyrics-posnercoref/pos_tagged_lyrics1/pos_tagged_lyrics.csv')\npos_df2 = pd.read_csv('/kaggle/input/lyrics-posnercoref/pos_tagged_lyrics_lastpart/pos_tagged_lyrics_lastpart.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:32:13.919581Z","iopub.execute_input":"2024-05-23T14:32:13.919957Z","iopub.status.idle":"2024-05-23T14:32:20.693428Z","shell.execute_reply.started":"2024-05-23T14:32:13.919927Z","shell.execute_reply":"2024-05-23T14:32:20.692368Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/951908246.py:5: DtypeWarning: Columns (2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n  pos_df1 = pd.read_csv('/kaggle/input/lyrics-posnercoref/pos_tagged_lyrics1/pos_tagged_lyrics.csv')\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pos_df1.dropna(subset=['pos_tags'],inplace=True)\npos_df1.drop(columns=['Unnamed: 0'], inplace=True)\npos_df2.dropna(subset=['pos_tags'],inplace=True)\npos_df2.drop(columns=['Unnamed: 0'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:32:20.695084Z","iopub.execute_input":"2024-05-23T14:32:20.695403Z","iopub.status.idle":"2024-05-23T14:32:20.942589Z","shell.execute_reply.started":"2024-05-23T14:32:20.695376Z","shell.execute_reply":"2024-05-23T14:32:20.941362Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"pos_df2.dropna(subset=['pos_tags'],inplace=True)\npos_df2.drop(columns=['Unnamed: 0'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:32:20.943840Z","iopub.execute_input":"2024-05-23T14:32:20.944329Z","iopub.status.idle":"2024-05-23T14:32:21.027666Z","shell.execute_reply.started":"2024-05-23T14:32:20.944294Z","shell.execute_reply":"2024-05-23T14:32:21.026395Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"pos_df= pd.concat([pos_df1,pos_df2],ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:32:21.029641Z","iopub.execute_input":"2024-05-23T14:32:21.029969Z","iopub.status.idle":"2024-05-23T14:32:21.103757Z","shell.execute_reply.started":"2024-05-23T14:32:21.029942Z","shell.execute_reply":"2024-05-23T14:32:21.102765Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"pos_merged_df = pd.merge(left=pos_df,right=merged_df[['lyric','Region','decade','Country']],on='lyric',how='left')\npos_merged_df.drop_duplicates(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:33:05.330689Z","iopub.execute_input":"2024-05-23T14:33:05.330960Z","iopub.status.idle":"2024-05-23T14:33:08.646726Z","shell.execute_reply.started":"2024-05-23T14:33:05.330937Z","shell.execute_reply":"2024-05-23T14:33:08.645597Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"sample_nr = 30\npos_samples = pos_merged_df.sample(sample_nr, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:33:08.648178Z","iopub.execute_input":"2024-05-23T14:33:08.648586Z","iopub.status.idle":"2024-05-23T14:33:08.698546Z","shell.execute_reply.started":"2024-05-23T14:33:08.648539Z","shell.execute_reply":"2024-05-23T14:33:08.697362Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"pos_samples = pd.read_csv('/kaggle/input/sampledata/pos_sampels.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:34:42.772637Z","iopub.execute_input":"2024-05-23T14:34:42.773064Z","iopub.status.idle":"2024-05-23T14:34:42.785608Z","shell.execute_reply.started":"2024-05-23T14:34:42.773034Z","shell.execute_reply":"2024-05-23T14:34:42.784049Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import ast\n\n# string to list \npos_samples['tokens'] = pos_samples['tokens'].apply(ast.literal_eval)\npos_samples['pos_tags'] = pos_samples['pos_tags'].apply(ast.literal_eval)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntoken_pos_pairs = []\nfor idx,row in pos_samples.iterrows():\n    tokens = row['tokens']\n    pos_tags = row['pos_tags']\n    \n    token_pos = list(zip(tokens,pos_tags))\n    token_pos_pairs.append(token_pos)\n    print(token_pos)\npos_samples['token_pos_pairs'] = token_pos_pairs","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:41:09.084654Z","iopub.execute_input":"2024-05-23T14:41:09.085078Z","iopub.status.idle":"2024-05-23T14:41:09.096174Z","shell.execute_reply.started":"2024-05-23T14:41:09.085042Z","shell.execute_reply":"2024-05-23T14:41:09.095333Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[('<s>', 'PROPN'), ('on', 'ADP'), ('beats', 'NOUN'), ('im', 'PRON'), ('a', 'DET'), ('beast', 'NOUN'), ('like', 'ADP'), ('to@@', 'PROPN'), ('to', 'PROPN'), ('ri@@', 'PROPN'), ('ina', 'PROPN'), ('</s>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN')]\n[('<s>', 'NOUN'), ('get', 'VERB'), ('from', 'ADP'), ('out', 'ADP'), ('the', 'DET'), ('row', 'NOUN'), ('when', 'SCONJ'), ('he', 'PRON'), ('get', 'VERB'), ('dough', 'NOUN'), ('its', 'PRON'), ('horrible', 'ADJ'), ('</s>', 'NOUN')]\n[('<s>', 'NOUN'), ('these', 'DET'), ('bitches', 'NOUN'), ('know', 'VERB'), ('im', 'PRON'), ('worth', 'ADJ'), ('these', 'DET'), ('comm@@', 'NOUN'), ('as', 'NOUN'), ('</s>', 'NOUN')]\n[('<s>', 'NOUN'), ('my', 'PRON'), ('stee@@', 'NOUN'), ('ze', 'NOUN'), ('is', 'AUX'), ('crazy', 'ADJ'), ('dog', 'NOUN'), ('</s>', 'PUNCT')]\n[('<s>', 'NOUN'), ('spea@@', 'VERB'), ('kin', 'NOUN'), ('of', 'ADP'), ('boat', 'NOUN'), ('i', 'PRON'), ('think', 'VERB'), ('right', 'ADV'), ('now', 'ADV'), ('hes', 'PRON'), ('with', 'ADP'), ('digital', 'ADJ'), ('nas', 'NOUN'), ('</s>', 'NOUN'), ('<pad>', 'NOUN')]\n[('<s>', 'NOUN'), ('im', 'PRON'), ('a', 'DET'), ('first', 'ADJ'), ('staff', 'NOUN'), ('og', 'NOUN'), ('from', 'ADP'), ('outta', 'ADP'), ('the', 'DET'), ('gutter', 'NOUN'), ('</s>', 'NOUN')]\n[('<s>', 'NOUN'), ('2020', 'NUM'), ('im', 'PRON'), ('lookin', 'VERB'), ('through', 'ADP'), ('a', 'DET'), ('tunnel', 'NOUN'), ('</s>', 'PUNCT')]\n[('<s>', 'PROPN'), ('go', 'VERB'), ('up', 'ADV'), ('go', 'VERB'), ('up', 'ADV'), ('go', 'VERB'), ('up', 'ADV'), ('</s>', 'PUNCT')]\n[('<s>', 'PROPN'), ('staying', 'VERB'), ('hungry', 'ADJ'), ('but', 'CCONJ'), ('your', 'PRON'), ('fear', 'NOUN'), ('is', 'AUX'), ('what', 'PRON'), ('i', 'PRON'), ('fed', 'VERB'), ('on', 'ADP'), ('</s>', 'PROPN')]\n[('<s>', 'PROPN'), ('ooh', 'INTJ'), ('look', 'VERB'), ('at', 'ADP'), ('big', 'ADJ'), ('momma', 'NOUN'), ('ooh', 'INTJ'), ('big', 'ADJ'), ('mama', 'NOUN'), ('</s>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN')]\n[('<s>', 'VERB'), ('all', 'DET'), ('we', 'PRON'), ('know', 'VERB'), ('is', 'AUX'), ('who', 'PRON'), ('they', 'PRON'), ('ridicul@@', 'VERB'), ('ed', 'VERB'), ('and', 'CCONJ'), ('who', 'PRON'), ('gets', 'AUX'), ('minis@@', 'VERB'), ('cul@@', 'NOUN'), ('ed', 'VERB'), ('</s>', 'VERB')]\n[('<s>', 'NOUN'), ('but', 'CCONJ'), ('i', 'PRON'), ('got', 'VERB'), ('a', 'DET'), ('ghetto', 'NOUN'), ('plaque', 'NOUN'), ('for', 'SCONJ'), ('showing', 'VERB'), ('love', 'NOUN'), ('to', 'ADP'), ('every', 'DET'), ('hood', 'NOUN'), ('</s>', 'NOUN'), ('<pad>', 'NOUN')]\n[('<s>', 'PROPN'), ('but', 'CCONJ'), ('please', 'INTJ'), ('dont', 'AUX'), ('take', 'VERB'), ('me', 'PRON'), ('for', 'ADP'), ('simple', 'ADJ'), ('</s>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN'), ('<pad>', 'PROPN')]\n[('<s>', 'PRON'), ('why', 'ADV'), ('do', 'AUX'), ('you', 'PRON'), ('think', 'VERB'), ('that', 'SCONJ'), ('you', 'PRON'), ('are', 'AUX'), ('all', 'DET'), ('that', 'SCONJ'), ('you', 'PRON'), ('aint', 'VERB'), ('</s>', 'PRON'), ('<pad>', 'PRON'), ('<pad>', 'PRON'), ('<pad>', 'PRON')]\n[('<s>', 'PROPN'), ('nah', 'INTJ'), ('nah', 'INTJ'), ('nah', 'INTJ'), ('nah', 'INTJ'), ('</s>', 'PUNCT')]\n[('<s>', 'PROPN'), ('darkest', 'ADJ'), ('secrets', 'NOUN'), ('that', 'PRON'), ('your', 'PRON'), ('shadow', 'NOUN'), ('will', 'AUX'), ('tell', 'VERB'), ('you', 'PRON'), ('</s>', 'NOUN')]\n[('<s>', 'PROPN'), ('i', 'PRON'), ('like', 'VERB'), ('sha@@', 'PROPN'), ('ggy', 'PROPN'), ('hes', 'PRON'), ('my', 'PRON'), ('friend', 'NOUN'), ('</s>', 'PROPN')]\n[('<s>', 'PRON'), ('you', 'PRON'), ('understand', 'VERB'), ('me', 'PRON'), ('boy', 'NOUN'), ('</s>', 'PUNCT'), ('<pad>', 'PRON'), ('<pad>', 'PRON'), ('<pad>', 'PRON'), ('<pad>', 'PRON'), ('<pad>', 'PRON'), ('<pad>', 'PRON'), ('<pad>', 'PRON'), ('<pad>', 'PRON'), ('<pad>', 'PRON'), ('<pad>', 'PRON'), ('<pad>', 'PRON')]\n[('<s>', 'PRON'), ('i', 'PRON'), ('will', 'AUX'), ('always', 'ADV'), ('place', 'VERB'), ('your', 'PRON'), ('memory', 'NOUN'), ('</s>', 'PUNCT')]\n[('<s>', 'PROPN'), ('got', 'VERB'), ('a', 'DET'), ('picture', 'NOUN'), ('of', 'ADP'), ('mal@@', 'PROPN'), ('colm', 'PROPN'), ('x', 'PROPN'), ('on', 'ADP'), ('the', 'DET'), ('wall', 'NOUN'), ('in', 'ADP'), ('my', 'PRON'), ('room', 'NOUN'), ('bitch', 'NOUN'), ('</s>', 'PROPN')]\n[('<s>', 'NOUN'), ('im', 'PRON'), ('taking', 'VERB'), ('all', 'DET'), ('shine', 'NOUN'), ('off', 'ADP'), ('top', 'NOUN'), ('</s>', 'NOUN')]\n[('<s>', 'PRON'), ('i', 'PRON'), ('will', 'AUX'), ('spray', 'VERB'), ('yall', 'PRON'), ('niggas', 'NOUN'), ('will', 'AUX'), ('waste', 'VERB'), ('yall', 'PRON'), ('niggas', 'NOUN'), ('</s>', 'PRON')]\n[('<s>', 'NOUN'), ('stim@@', 'VERB'), ('ulates', 'VERB'), ('the', 'DET'), ('opposite', 'ADJ'), ('sets', 'NOUN'), ('and', 'CCONJ'), ('thats', 'PRON'), ('dope', 'ADJ'), ('</s>', 'NOUN')]\n[('<s>', 'NOUN'), ('theres', 'PRON'), ('some', 'DET'), ('type', 'NOUN'), ('of', 'ADP'), ('way', 'NOUN'), ('kid', 'NOUN'), ('were', 'PRON'), ('all', 'ADV'), ('related', 'ADJ'), ('</s>', 'NOUN')]\n[('<s>', 'PRON'), ('hey', 'INTJ'), ('yall', 'PRON'), ('know', 'VERB'), ('my', 'PRON'), ('mothers', 'NOUN'), ('sickness', 'NOUN'), ('yall', 'PRON'), ('know', 'VERB'), ('im', 'PRON'), ('breakin', 'VERB'), ('down', 'ADP'), ('</s>', 'PRON')]\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"pos_samples.to_csv('pos_sampels.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T14:42:38.690471Z","iopub.execute_input":"2024-05-23T14:42:38.690861Z","iopub.status.idle":"2024-05-23T14:42:38.701452Z","shell.execute_reply.started":"2024-05-23T14:42:38.690829Z","shell.execute_reply":"2024-05-23T14:42:38.700239Z"},"trusted":true},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"### NER Tag Extraction","metadata":{}},{"cell_type":"code","source":"\nner_df = pd.read_csv('/kaggle/input/lyrics-posnercoref/ner_tagged_lyrics/kaggle/working/ner_tagged_lyrics.csv')\nner_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:04:02.137042Z","iopub.execute_input":"2024-05-23T12:04:02.137438Z","iopub.status.idle":"2024-05-23T12:04:08.184626Z","shell.execute_reply.started":"2024-05-23T12:04:02.137408Z","shell.execute_reply":"2024-05-23T12:04:08.183468Z"},"trusted":true},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                            lyric  \\\n0           0                                  rgf productions   \n1           1                                  remy boyz yahah   \n2           2                                         1738 ayy   \n3           3                      im like hey whats up hello    \n4           4  seen yo pretty ass soon as you came in the door   \n\n                                              tokens  \\\n0  ['[CLS]', 'r', '##gf', 'productions', '[SEP]',...   \n1  ['[CLS]', 'remy', 'boy', '##z', 'ya', '##ha', ...   \n2  ['[CLS]', '1738', 'a', '##y', '##y', '[SEP]', ...   \n3  ['[CLS]', 'im', 'like', 'hey', 'what', '##s', ...   \n4  ['[CLS]', 'seen', 'yo', 'pretty', 'ass', 'soon...   \n\n                                            ner_tags  \n0  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n1  ['O', 'B-person', 'O', 'O', 'O', 'O', 'O', 'O'...  \n2  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>lyric</th>\n      <th>tokens</th>\n      <th>ner_tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>rgf productions</td>\n      <td>['[CLS]', 'r', '##gf', 'productions', '[SEP]',...</td>\n      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>remy boyz yahah</td>\n      <td>['[CLS]', 'remy', 'boy', '##z', 'ya', '##ha', ...</td>\n      <td>['O', 'B-person', 'O', 'O', 'O', 'O', 'O', 'O'...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1738 ayy</td>\n      <td>['[CLS]', '1738', 'a', '##y', '##y', '[SEP]', ...</td>\n      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>im like hey whats up hello</td>\n      <td>['[CLS]', 'im', 'like', 'hey', 'what', '##s', ...</td>\n      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>seen yo pretty ass soon as you came in the door</td>\n      <td>['[CLS]', 'seen', 'yo', 'pretty', 'ass', 'soon...</td>\n      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"ner_df.ner_tags.values[0:2]","metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:05:23.993639Z","iopub.execute_input":"2024-05-23T12:05:23.994032Z","iopub.status.idle":"2024-05-23T12:05:24.001533Z","shell.execute_reply.started":"2024-05-23T12:05:23.994004Z","shell.execute_reply":"2024-05-23T12:05:24.000322Z"},"trusted":true},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"array([\"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\",\n       \"['O', 'B-person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\"],\n      dtype=object)"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"ner_df.dropna(inplace=True)\nner_df.drop(columns=['Unnamed: 0'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:11:24.341931Z","iopub.execute_input":"2024-05-23T12:11:24.342323Z","iopub.status.idle":"2024-05-23T12:11:24.649271Z","shell.execute_reply.started":"2024-05-23T12:11:24.342286Z","shell.execute_reply":"2024-05-23T12:11:24.648130Z"},"trusted":true},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# Convert string representations to actual lists\nner_df_subset = ner_df.copy()\nner_df_subset['ner_tags'] = ner_df_subset['ner_tags'].apply(ast.literal_eval)\n\ndef has_all_o_tags(tag_list):\n    return all(tag == 'O' for tag in tag_list)\n\nner_df_subset_all_os = ner_df_subset[ner_df_subset['ner_tags'].apply(has_all_o_tags)]","metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:22:42.455727Z","iopub.execute_input":"2024-05-23T12:22:42.456136Z","iopub.status.idle":"2024-05-23T12:23:32.505728Z","shell.execute_reply.started":"2024-05-23T12:22:42.456107Z","shell.execute_reply":"2024-05-23T12:23:32.504663Z"},"trusted":true},"outputs":[],"execution_count":89},{"cell_type":"code","source":"import ast\n\n\n# Convert string representations to actual lists\nner_df_subset = ner_df.copy()\nner_df_subset['ner_tags'] = ner_df_subset['ner_tags'].apply(ast.literal_eval)\n\ndef has_non_o_tags(tag_list):\n    return any(tag != 'O' for tag in tag_list)\n\nner_df_subset_no_os = ner_df_subset[ner_df_subset['ner_tags'].apply(has_non_o_tags)]","metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:17:20.808183Z","iopub.execute_input":"2024-05-23T12:17:20.808523Z","iopub.status.idle":"2024-05-23T12:18:10.447053Z","shell.execute_reply.started":"2024-05-23T12:17:20.808495Z","shell.execute_reply":"2024-05-23T12:18:10.445706Z"},"trusted":true},"outputs":[],"execution_count":70},{"cell_type":"code","source":"rounded_proportion = round(len(ner_df_subset_no_os) / len(ner_df), 2)\nprint(f'Proportion of entries where at least a Tag was specified: {rounded_proportion}')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:21:44.997366Z","iopub.execute_input":"2024-05-23T12:21:44.997752Z","iopub.status.idle":"2024-05-23T12:21:45.002723Z","shell.execute_reply.started":"2024-05-23T12:21:44.997724Z","shell.execute_reply":"2024-05-23T12:21:45.001858Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Proportion of entries where at least a Tag was specified: 0.1\n","output_type":"stream"}],"execution_count":85},{"cell_type":"markdown","source":"#### Quite imbalanced which is to be expected.","metadata":{}},{"cell_type":"code","source":"#sample 15 entires from set containing at least one Non-O tag (since otherwise imbalancee makes it not the best samples), and 15 with only Os ( just a way to better evaluate imbalanced dataset)\ngeneral_sample_nr = 15\n\nall_os_sample_df = ner_df_subset_all_os.sample(general_sample_nr,random_state=42)\nno_os_sample_df = ner_df_subset_no_os.sample(general_sample_nr,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:25:21.045757Z","iopub.execute_input":"2024-05-23T12:25:21.046169Z","iopub.status.idle":"2024-05-23T12:25:21.089205Z","shell.execute_reply.started":"2024-05-23T12:25:21.046138Z","shell.execute_reply":"2024-05-23T12:25:21.087854Z"},"trusted":true},"outputs":[],"execution_count":91},{"cell_type":"code","source":"ner_samples_all = pd.concat([all_os_sample_df,no_os_sample_df],ignore_index=True)\nner_samples_all.to_csv('ner_samples.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:25:55.297208Z","iopub.execute_input":"2024-05-23T12:25:55.297663Z","iopub.status.idle":"2024-05-23T12:25:55.304233Z","shell.execute_reply.started":"2024-05-23T12:25:55.297628Z","shell.execute_reply":"2024-05-23T12:25:55.302885Z"},"trusted":true},"outputs":[],"execution_count":93},{"cell_type":"markdown","source":"#### Coreference Sample Extraction","metadata":{}},{"cell_type":"code","source":"coref_df = pd.read_csv('/kaggle/input/lyrics-posnercoref/full_coref/full_coref.csv')\ncoref_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:27:24.904442Z","iopub.execute_input":"2024-05-23T12:27:24.904892Z","iopub.status.idle":"2024-05-23T12:27:27.846550Z","shell.execute_reply.started":"2024-05-23T12:27:24.904857Z","shell.execute_reply":"2024-05-23T12:27:27.845387Z"},"trusted":true},"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                            lyric  \\\n0           0                                  rgf productions   \n1           1                                  remy boyz yahah   \n2           2                                         1738 ayy   \n3           3                      im like hey whats up hello    \n4           4  seen yo pretty ass soon as you came in the door   \n\n                                    resolved_lyric  \n0                                  rgf productions  \n1                                  remy boyz yahah  \n2                                         1738 ayy  \n3                      im like hey whats up hello   \n4  seen yo pretty ass soon as you came in the door  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>lyric</th>\n      <th>resolved_lyric</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>rgf productions</td>\n      <td>rgf productions</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>remy boyz yahah</td>\n      <td>remy boyz yahah</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1738 ayy</td>\n      <td>1738 ayy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>im like hey whats up hello</td>\n      <td>im like hey whats up hello</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>seen yo pretty ass soon as you came in the door</td>\n      <td>seen yo pretty ass soon as you came in the door</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":96},{"cell_type":"code","source":"coref_df.dropna(inplace=True)\ncoref_df.drop(columns=['Unnamed: 0'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:28:47.304720Z","iopub.execute_input":"2024-05-23T12:28:47.305102Z","iopub.status.idle":"2024-05-23T12:28:47.547150Z","shell.execute_reply.started":"2024-05-23T12:28:47.305074Z","shell.execute_reply":"2024-05-23T12:28:47.545973Z"},"trusted":true},"outputs":[],"execution_count":101},{"cell_type":"code","source":"coref_changes_made = coref_df.loc[coref_df['lyric']!=coref_df['resolved_lyric']]","metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:29:56.528143Z","iopub.execute_input":"2024-05-23T12:29:56.528556Z","iopub.status.idle":"2024-05-23T12:29:56.654304Z","shell.execute_reply.started":"2024-05-23T12:29:56.528527Z","shell.execute_reply":"2024-05-23T12:29:56.653163Z"},"trusted":true},"outputs":[],"execution_count":104},{"cell_type":"code","source":"rounded_proportion = round(len(coref_changes_made) / len(coref_df), 5)\nprint(f'Proportion of entries where some coreferences were resolved: {rounded_proportion}')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:30:42.185607Z","iopub.execute_input":"2024-05-23T12:30:42.185993Z","iopub.status.idle":"2024-05-23T12:30:42.192063Z","shell.execute_reply.started":"2024-05-23T12:30:42.185964Z","shell.execute_reply":"2024-05-23T12:30:42.190665Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Proportion of entries where some coreferences were resolved: 0.00407\n","output_type":"stream"}],"execution_count":109},{"cell_type":"markdown","source":"#### Only 0.4% of the text had some coreferences that were identified and needed to be resolved. This extreme imbalance once again makes me believe it's important to manually test changed rows, as well as ones that had no change.","metadata":{}},{"cell_type":"code","source":"# extract samples only from subset where changes have been made\n\ngeneral_sample_nr = 15\nsample_changes_made = coref_changes_made.sample(general_sample_nr,random_state=42)\nsample_no_changes_made = coref_df.loc[~coref_df.index.isin(coref_changes_made.index)].sample(general_sample_nr,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:34:45.908255Z","iopub.execute_input":"2024-05-23T12:34:45.908670Z","iopub.status.idle":"2024-05-23T12:34:45.915109Z","shell.execute_reply.started":"2024-05-23T12:34:45.908630Z","shell.execute_reply":"2024-05-23T12:34:45.913994Z"},"trusted":true},"outputs":[],"execution_count":110},{"cell_type":"code","source":"sample_changes_made['Type'] = 'changes made'\nsample_no_changes_made['Type'] = 'no changes made'","metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:36:15.089656Z","iopub.execute_input":"2024-05-23T12:36:15.090275Z","iopub.status.idle":"2024-05-23T12:36:15.098468Z","shell.execute_reply.started":"2024-05-23T12:36:15.090233Z","shell.execute_reply":"2024-05-23T12:36:15.097002Z"},"trusted":true},"outputs":[],"execution_count":113},{"cell_type":"code","source":"coref_samples_all = pd.concat([sample_changes_made,sample_no_changes_made],ignore_index=True)\ncoref_samples_all.to_csv('coref_samples_all.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:36:21.706602Z","iopub.execute_input":"2024-05-23T12:36:21.707144Z","iopub.status.idle":"2024-05-23T12:36:21.714751Z","shell.execute_reply.started":"2024-05-23T12:36:21.707104Z","shell.execute_reply":"2024-05-23T12:36:21.713550Z"},"trusted":true},"outputs":[],"execution_count":114},{"cell_type":"markdown","source":"### After Human Validation ","metadata":{}},{"cell_type":"markdown","source":"### POS Tags","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import cohen_kappa_score\n\n\n\npos_dan = pd.read_excel('/kaggle/input/validatedsamples/pos_samples_dan.xlsx')\npos_cristina = pd.read_excel('/kaggle/input/validatedsamples/pos_samples_cristina.xlsx')\n\npos_dan['person'] = 'dan'\npos_cristina['person'] = 'cristina'\n\npos_dan.columns = pos_cristina.columns\npos_df = pd.concat([pos_dan, pos_cristina], ignore_index=True)\n\n\npos_df['Human Classification'] = pos_df['Human Classification'].apply(ast.literal_eval)\npos_df['pos_tags_human'] = pos_df['Human Classification'].apply(lambda x: [tag for _, tag in x])\npos_df['pos_tags'] = pos_df['pos_tags'].apply(ast.literal_eval)\n\n\n\nrater1 = pos_df[pos_df['person'] == 'dan']['pos_tags_human'].values\nrater2 = pos_df[pos_df['person'] == 'cristina']['pos_tags_human'].values\n\nmin_length = min(len(rater1), len(rater2))\nrater1 = rater1[:min_length]\nrater2 = rater2[:min_length]\n\nrater1_flat = [tag for tags_list in rater1 for tag in tags_list]\nrater2_flat = [tag for tags_list in rater2 for tag in tags_list]\nkappa = cohen_kappa_score(rater1_flat, rater2_flat)\n\nprint(f\"POS Tagging: Cohen's kappa: {kappa}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T10:15:15.918983Z","iopub.execute_input":"2024-05-27T10:15:15.919384Z","iopub.status.idle":"2024-05-27T10:15:15.981276Z","shell.execute_reply.started":"2024-05-27T10:15:15.919352Z","shell.execute_reply":"2024-05-27T10:15:15.979980Z"},"trusted":true},"outputs":[{"name":"stdout","text":"POS Tagging: Cohen's kappa: 0.954393659971091\n","output_type":"stream"}],"execution_count":123},{"cell_type":"code","source":"# Function to count common elements preserving duplicates\ndef count_common_elements(list1, list2):\n    common_count = 0\n    list2_copy = list2[:]\n    for item in list1:\n        if item in list2_copy:\n            common_count += 1\n            list2_copy.remove(item)\n    return common_count\n\n# Determine the proportion of common elements per row\npos_df['pos_accuracy'] = pos_df.apply(\n    lambda row: count_common_elements(row['pos_tags_human'], row['pos_tags']) / max(len(row['pos_tags_human']), len(row['pos_tags'])),\n    axis=1\n)\n\n# Group by 'person' and calculate the mean accuracy for each person\ngrouped_accuracy = pos_df.groupby('person')['pos_accuracy'].mean().reset_index()\nprint(f'POS Tagging: Average Accuracy: {grouped_accuracy[\"pos_accuracy\"].mean()}')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T10:15:18.018964Z","iopub.execute_input":"2024-05-27T10:15:18.019372Z","iopub.status.idle":"2024-05-27T10:15:18.032837Z","shell.execute_reply.started":"2024-05-27T10:15:18.019339Z","shell.execute_reply":"2024-05-27T10:15:18.031656Z"},"trusted":true},"outputs":[{"name":"stdout","text":"POS Tagging: Average Accuracy: 0.9679382284382284\n","output_type":"stream"}],"execution_count":124},{"cell_type":"markdown","source":"#### NER","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import cohen_kappa_score\n\n\n\nner_dan = pd.read_excel('/kaggle/input/validatedsamples/ner_samples_dan.xlsx')\nner_cristina = pd.read_excel('/kaggle/input/validatedsamples/ner_samples_gicu.xlsx')\n\nner_dan['person'] = 'dan'\nner_cristina['person'] = 'gicu'\n\nner_dan.columns = ner_cristina.columns\nner_df = pd.concat([ner_dan, ner_cristina], ignore_index=True)\n\n\n\nner_df['Human Classification'] = ner_df['Human Classification'].apply(ast.literal_eval)\n#ner_df['pos_tags_human'] = ner_df['Human Classification'].apply(lambda x: [tag for _, tag in x])\nner_df['ner_tags'] = ner_df['ner_tags'].apply(ast.literal_eval)\n\n\n\nrater1 = ner_df[ner_df['person'] == 'dan']['Human Classification'].values\nrater2 = ner_df[ner_df['person'] == 'gicu']['Human Classification'].values\n\nmin_length = min(len(rater1), len(rater2))\n\nrater1_flat = [tag for tags_list in rater1 for tag in tags_list]\nrater2_flat = [tag for tags_list in rater2 for tag in tags_list]\n\n\n\nkappa = cohen_kappa_score(rater1_flat, rater2_flat)\n\nprint(f\"NER: Cohen's kappa: {kappa}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T10:18:13.264125Z","iopub.execute_input":"2024-05-27T10:18:13.264513Z","iopub.status.idle":"2024-05-27T10:18:13.321202Z","shell.execute_reply.started":"2024-05-27T10:18:13.264484Z","shell.execute_reply":"2024-05-27T10:18:13.319874Z"},"trusted":true},"outputs":[{"name":"stdout","text":"NER: Cohen's kappa: 0.8906038339969623\n","output_type":"stream"}],"execution_count":139},{"cell_type":"code","source":"# Function to count common elements preserving duplicates\ndef count_common_elements(list1, list2):\n    common_count = 0\n    list2_copy = list2[:]\n    for item in list1:\n        if item in list2_copy:\n            common_count += 1\n            list2_copy.remove(item)\n    return common_count\n\n# Determine the proportion of common elements per row\nner_df['ner_accuracy'] = ner_df.apply(\n    lambda row: count_common_elements(row['Human Classification'], row['ner_tags']) / max(len(row['Human Classification']), len(row['ner_tags'])),\n    axis=1\n)\n\n# Group by 'person' and calculate the mean accuracy for each person\ngrouped_accuracy = ner_df.groupby('person')['ner_accuracy'].mean().reset_index()\nprint(f'NER: Average Accuracy: {grouped_accuracy[\"ner_accuracy\"].mean()}')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T10:19:16.767864Z","iopub.execute_input":"2024-05-27T10:19:16.768274Z","iopub.status.idle":"2024-05-27T10:19:16.783347Z","shell.execute_reply.started":"2024-05-27T10:19:16.768240Z","shell.execute_reply":"2024-05-27T10:19:16.781916Z"},"trusted":true},"outputs":[{"name":"stdout","text":"NER: Average Accuracy: 0.9848420069782299\n","output_type":"stream"}],"execution_count":141}]}